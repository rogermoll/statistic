{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "## Classification\nClassification is perhaps the most important for of prediction: it is to predict whether a record it 0 or 1. Rather than having a model simply assigning a binary classification, most algorithms can return a probability score of belonging to the class of interest.\n\nFor a binary respons Y = i (i = 0 or 1), estimate the individual conditional probabilities for each predictor $P(X_j | Y = i)$: these are the probabilities that the predictor value is in the recort when we observe Y=i.\n\n#### Covariance Matrix\nIntroduction of the concept of covariance between two or more variables. The covariance measures the relationship between two vairables x and z.\n\n$s_{x,z} = \\frac{\\sum_{i=1}^{n}{(x_i-\\overline{x})(z_i-\\overline{z})}}{(n-1)}$  \n\n$\\overline{x}$ = mean of x (e.z. z)\n\nCorrelation is constraint to between -1 and 1, whereas covariance is on the same scale as the vairables x and z. The covariance matrix $\\sum$ for x and z consists of the individual variable variances $s_x^2$ and $s_y^2$ on the *diagonal* and the covariances between variable pairs on the *off-diagonals*.  \n\n$\\hat{\\sum} = \\begin{bmatrix} \n                s_x^2 & s_{x,z}\\\\\n                s_{z,x} & s_z^2\n                \\end{bmatrix}$\n\n### Discriminant Analysis\n**Key Terms**\n* Covariance: A measure of the extend to which one variable varies in concert with another.\n* Discriminant function: The function that, when appplied to the predictor vaiables, maximizes the separation of the classes.\n* Discriminant weights: The scores that results from the application of the discriminant function, and are used to estimate probabilities of belonging to one class or another.\n\n### Linear discriminant analysis (LDA)\nDiscriminant analysis assumes the predictor varbiables aer normally distributed continues variables. In practice, the method works well even for nonextrem departures from normality and for binary predictors. Fisher's linear discriminanat distinguishes variation between the groups from within the groups. It seeks to maximise the \"between\" sum of squares $SS_{between}$ relative to the \"within\" sum of squares $ss_{within}$. The method finds the linear combination of $w_xx + w_zz$ that maximises the sum of square ratio.  \n\n$\\frac{ss_{between}}{ss_{within}}$  \n\nThe  $SS_{between}$ is the squared distance between the two group means, and the $ss_{within}$ is the spread around the means within each group, weighted by the covariane matrix. This function yields the gratest separation between the groups by maximising $SS_{between}$ and minimising $ss_{within}$.  \n\nUsing the discriminat function weights, LDA splits teh predictor space into two regions.\n\n### Logistic Regression\nLogistic regression is analgous to multiple regression except that the outcome is binary. It is a structures approach, rather than a data-centric approach.  \n**Key Terms**\n* Logit: The function that maps the probability of belonging to a class with a ranger from $+/- \\infty$ (instead of 0 to 1).\n* Odds: the ratio of success (1) to not success (0)\n* Log odds: The respone in the transformed model (now linear), which gets mapped back to a probability\n\nThe key is the logistic response function and the logit, in which we map a probability to a more expansive scale. Think of the outcome variable not as a binary label but as the probabilty $p$ that the lable is a \"1\". Model $p$ by applying a logistic response function to the predictor variables  \n\n$p = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}}$  This transformation ensures that $p$ stays between 0 and 1.  \n\nOdds(Y=1) = $\\frac{p}{1-p}$ = ratio of success(1) to nonsuccess (0) = $e^{(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}$.  \nlog(Odds) = $\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$\n\nProbability $p = \\frac{Odds}{1+Odds}$\n\nThe log-odds function, known as *logit* function, maps the probability p from (0,1) to any value ($-\\infty , +\\infty$)  \n[logit function](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Logit.svg/525px-Logit.svg.png)\n\nLogistic regression is a special instance of a generaliesed linear model (GLM). GLMs are characterised by:\n* a probability distribution family\n* a link function mapping the response to the predictors\n\n**Predicted Values**  \nThe predicted values is given by the logistic response function: $\\hat{p} = \\frac{1}{1 + e^{-\\hat{Y}}}$  \n\nMultiple linear regression and logistic regression share many commonalities:\n* both assume a parametric liner form, relating the predictors with response\n* exploring and finding the best model are done in similar ways\n\nLogistic regression differs by:\n* the way the model is fit\n* the nature and analysis of the residuals from the model\n\n**Fitting the model** is done by using the maximum likelihood estimation (MLE). The MLE finds the solution such that the estimated log odds best describes the observed outcome.\n**Assessing the model** is done by assessing how accuratly the model classifies new data. Standard statistical tools to assess and improve the model's estimated coefficients are:\n* R, reporting the standard error of the coefficients (SE)\n* z-value\n* p-value\n\nHowever, a logist regression model which has a binary response, does not have an associated RMSE or R^2. Instead, a logistic regression model is evaluated using more general metrics for classification, i.e.\n* Accuracy: the percent (or proportion) of cases classified correctly\n* Confusion matrix: a tabular display (2x2) of the record counts by their predicted and actual classification status\n* Sensitivity: The percent (or proportion) of 1's correctly classified\n* Specifity: the percent (or proportion) of 0's correctly classified\n* Precision: the percent (or proportion) of predicted 1's that are actually 1's\n* ROC curve: A plot of sensitivity versus specifity\n* Lift: A measure of how effective the model is at identifying (comparitively rare) 1s at different probability cut offs\n\n**Accuracy**  \nCount of the proportion of predictions that are correct = $\\frac{\\sum{TruePos} + \\sum{TrueNeg}}{SampleSize}$  \n\n**Confusion Matrix**  \nA table showing the number of correct and incorrect predictions categorised by the type of response. The diagnoal elements of the matrix show the number of the matrix of correct predictions and the off-diagonal elements show the number of incorrect predidictions.  \n[Confusion Matrix](https://https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png?w=1200)  \n\n* precision = $\\frac{\\sum{TruePos}}{\\sum{TruePos} + \\sum{FalsePos}}$\n* recall = $\\frac{\\sum{TrueNeg}}{\\sum{TrueNeg} + \\sum{FalseNeg}}$\n* specifity = $\\frac{\\sum{TrueNeg}}{\\sum{TrueNEg} + \\sum{FalsePos}}$\n\n**Receiver Operating Characteristics (ROC)**  \nThe ROC curv plots recall on the y-axis against specifity on x-axis and shows the trade-off between recall and specifity. An extremly effective classifier will have an ROC that hugs the upper left corner.   \n[ROC](http://deparkes.co.uk/wp-content/uploads/2018/02/roc_curve_1.png)  \n\n### Strategies for Imbalanced Data\n**Key Terms**  \n* Undersample: use fewer of the prevalent class records in the classification model\n* Oversample: use more of the rare class records in the classification model, bootstraping if necessary\n* Up weight or down weight: attache more (or less) weight to the rare (or prevalent) class in the model\n* Data generation: Each new bootstrapped record is slightly different\n* z-Score: the value that results after standardisation\n* K: The number of neighbours considered in the nearest neighbour calculation\n\n## Statstical Machine Learning\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}