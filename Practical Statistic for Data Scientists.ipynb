{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "## Classification\nClassification is perhaps the most important for of prediction: it is to predict whether a record it 0 or 1. Rather than having a model simply assigning a binary classification, most algorithms can return a probability score of belonging to the class of interest.\n\nFor a binary respons Y = i (i = 0 or 1), estimate the individual conditional probabilities for each predictor $P(X_j | Y = i)$: these are the probabilities that the predictor value is in the recort when we observe Y=i.\n\n#### Covariance Matrix\nIntroduction of the concept of covariance between two or more variables. The covariance measures the relationship between two vairables x and z.\n\n$s_{x,z} = \\frac{\\sum_{i=1}^{n}{(x_i-\\overline{x})(z_i-\\overline{z})}}{(n-1)}$  \n\n$\\overline{x}$ = mean of x (e.z. z)\n\nCorrelation is constraint to between -1 and 1, whereas covariance is on the same scale as the vairables x and z. The covariance matrix $\\sum$ for x and z consists of the individual variable variances $s_x^2$ and $s_y^2$ on the *diagonal* and the covariances between variable pairs on the *off-diagonals*.  \n\n$\\hat{\\sum} = \\begin{bmatrix} \n                s_x^2 & s_{x,z}\\\\\n                s_{z,x} & s_z^2\n                \\end{bmatrix}$\n\n### Discriminant Analysis\n**Key Terms**\n* Covariance: A measure of the extend to which one variable varies in concert with another.\n* Discriminant function: The function that, when appplied to the predictor vaiables, maximizes the separation of the classes.\n* Discriminant weights: The scores that results from the application of the discriminant function, and are used to estimate probabilities of belonging to one class or another.\n\n### Linear discriminant analysis (LDA)\nDiscriminant analysis assumes the predictor varbiables aer normally distributed continues variables. In practice, the method works well even for nonextrem departures from normality and for binary predictors. Fisher's linear discriminanat distinguishes variation between the groups from within the groups. It seeks to maximise the \"between\" sum of squares $SS_{between}$ relative to the \"within\" sum of squares $ss_{within}$. The method finds the linear combination of $w_xx + w_zz$ that maximises the sum of square ratio.  \n\n$\\frac{ss_{between}}{ss_{within}}$  \n\nThe  $SS_{between}$ is the squared distance between the two group means, and the $ss_{within}$ is the spread around the means within each group, weighted by the covariane matrix. This function yields the gratest separation between the groups by maximising $SS_{between}$ and minimising $ss_{within}$.  \n\nUsing the discriminat function weights, LDA splits teh predictor space into two regions.\n\n### Logistic Regression\nLogistic regression is analgous to multiple regression except that the outcome is binary. It is a structures approach, rather than a data-centric approach.  \n**Key Terms**\n* Logit: The function that maps the probability of belonging to a class with a ranger from $+/- \\infty$ (instead of 0 to 1).\n* Odds: the ratio of success (1) to not success (0)\n* Log odds: The respone in the transformed model (now linear), which gets mapped back to a probability\n\nThe key is the logistic response function and the logit, in which we map a probability to a more expansive scale. Think of the outcome variable not as a binary label but as the probabilty $p$ that the lable is a \"1\". Model $p$ by applying a logistic response function to the predictor variables  \n\n$p = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}}$  This transformation ensures that $p$ stays between 0 and 1.  \n\nOdds(Y=1) = $\\frac{p}{1-p}$ = ratio of success(1) to nonsuccess (0) = $e^{(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}$.  \nlog(Odds) = $\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$\n\nProbability $p = \\frac{Odds}{1+Odds}$\n\nThe log-odds function, known as *logit* function, maps the probability p from (0,1) to any value ($-\\infty , +\\infty$)\n[logit function]{https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Logit.svg/525px-Logit.svg.png}\n\nLogistic regression is a special instance of a generaliesed linear model (GLM). GLMs are characterised by:\n* a probability distribution family\n* a link function mapping the response to the predictors\n\n\n## Statstical Machine Learning\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}